#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus=1

#SBATCH --job-name=NLP-SST2
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --time=00:59:00
#SBATCH --mem=60000M
#SBATCH --output=/home/mkrastev/FoMo-LoRA/%A.out

date

export HF_DATASETS_CACHE=/scratch-local/mkrastev/hf_cache_dir

WORK_DIR=$HOME/FoMo-LoRA
cd $WORK_DIR

module load 2023
module load PyTorch/2.1.2-foss-2023a-CUDA-12.1.1
source $WORK_DIR/.venv/bin/activate

TOKENIZERS_PARALLELISM=false python -m src.train --lora dynalora --schedule_type "periodic;100" --allocator_type "multinomial;10" --aggregate_type='l2' --output_dir output --task sst2 --epochs 60 --batch_size 64 --learning_rate 5e-4 --lora_alpha 8 --lora_r 8 --bf16 # --torch_compile